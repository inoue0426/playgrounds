{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48cdbb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6e7ec6",
   "metadata": {},
   "source": [
    "# Sparse Mixture of Experts (MoE)\n",
    "\n",
    "The Sparse Mixture of Experts model is an advanced neural network architecture that utilizes a set of specialized sub-models, known as \"experts,\" each of which is trained to handle specific types of data inputs. The \"sparse\" aspect of the model refers to the mechanism where only a subset of these experts is active for a given input, which helps in managing computational resources more efficiently.\n",
    "\n",
    "## Components\n",
    "\n",
    "### Gate\n",
    "The gate is a crucial component of the Sparse MoE model. It decides which experts are activated based on the input data. The gate's output is a distribution over experts, typically determined by a softmax function:\n",
    "\n",
    "$$ \\text{Gate Outputs} = \\text{softmax}(W_g \\cdot x + b_g) $$\n",
    "\n",
    "Where $ W_g $ and $ b_g $ are the weights and biases of the gate, and $ x $ is the input vector.\n",
    "\n",
    "### Experts\n",
    "Each expert is a neural network designed to process specific kinds of information. In a Sparse MoE, each expert operates independently on the input when activated:\n",
    "\n",
    "$$ \\text{Expert Output}_i = f_i(x) $$\n",
    "\n",
    "Where $ f_i $ represents the function modeled by the i-th expert.\n",
    "\n",
    "## Aggregation\n",
    "The outputs of the active experts are aggregated based on the weights assigned by the gate:\n",
    "\n",
    "$$ \\text{Output} = \\sum_{i=1}^{N} w_i \\cdot \\text{Expert Output}_i $$\n",
    "\n",
    "Where $ w_i $ are the weights from the gate outputs, and $ N $ is the total number of experts.\n",
    "\n",
    "## Benefits\n",
    "- **Efficiency**: By only activating a subset of experts, Sparse MoE models can handle larger models and datasets more efficiently than dense architectures.\n",
    "- **Scalability**: It is straightforward to add more experts to the system to improve its capacity and performance.\n",
    "- **Flexibility**: Experts can be trained on different tasks, making the model adaptable to a wide range of applications.\n",
    "\n",
    "## Applications\n",
    "Sparse MoE models are particularly useful in scenarios where computational resources are limited, or the data exhibits high variability requiring specialized handling. They are widely used in fields like natural language processing and computer vision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0276ace6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Expert, self).__init__()\n",
    "        self.layer = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14c6cf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseMoE(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_experts):\n",
    "        super(SparseMoE, self).__init__()\n",
    "        self.gate = nn.Linear(input_dim, num_experts)\n",
    "        self.experts = nn.ModuleList([Expert(input_dim, output_dim) for _ in range(num_experts)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        gates = self.gate(x)\n",
    "        expert_weights = torch.softmax(gates, dim=1)\n",
    "        outputs = torch.stack([expert(x) for expert in self.experts])\n",
    "        return torch.sum(outputs * expert_weights.unsqueeze(-1), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbb94590",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 10\n",
    "output_dim = 5\n",
    "num_experts = 2\n",
    "\n",
    "model = SparseMoE(input_dim, output_dim, num_experts)\n",
    "x = torch.randn(1, input_dim)\n",
    "output = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b81e9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4241,  0.7942, -0.4629,  0.0527, -0.0950],\n",
       "        [-0.7232,  1.3545, -0.7894,  0.0899, -0.1620]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c4af05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
