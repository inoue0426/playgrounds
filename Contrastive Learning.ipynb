{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "satellite-hostel",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kuangr/inoue019/.conda/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from tqdm import auto\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-ontario",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "## Siamese Network Structure\n",
    "\n",
    "In a Siamese Network, two input images $ x_1 $ and $ x_2 $ are processed through the same network function $ f $, which extracts feature vectors from each image. This process can be mathematically represented as follows:\n",
    "\n",
    "$v_i = f(x_i)$\n",
    "\n",
    "where $ v_i $ is the feature vector extracted from the image $ x_i $.\n",
    "\n",
    "## Contrastive Loss\n",
    "\n",
    "The Contrastive Loss function is designed to train the model such that feature vectors of similar image pairs are brought closer together, while those of dissimilar pairs are pushed apart. This is mathematically formulated as:\n",
    "\n",
    "$ L(v_1, v_2, y) = (1 - y) \\cdot \\|v_1 - v_2\\|^2 + y \\cdot \\max(0, m - \\|v_1 - v_2\\|)^2 $\n",
    "\n",
    "where:\n",
    "- $ v_1 $ and $ v_2 $ are the feature vectors extracted from images $ x_1 $ and $ x_2 $, respectively.\n",
    "- $ y $ is the label, which is 0 if the images belong to the same class and 1 if they belong to different classes.\n",
    "- $ m $ is the margin, a hyperparameter that defines how far apart the feature vectors of dissimilar pairs should be.\n",
    "- $ \\|v_1 - v_2\\| $ represents the Euclidean distance between $ v_1 $ and $ v_2 $.\n",
    "\n",
    "This loss function encourages the network to minimize the distance between feature vectors of similar pairs and ensure that the distance between dissimilar pairs is at least the margin $ m $.\n",
    "\n",
    "## Implementation Notes\n",
    "\n",
    "In the implementation, a pretrained network such as ResNet18 can be utilized as the feature extractor, with modifications made to the final layer to adjust the dimensionality of the feature space. The network outputs $ v_i $, which are used to compute the Contrastive Loss. The model is trained through backpropagation to update the weights of the network based on this loss.\n",
    "\n",
    "---\n",
    "\n",
    "This Markdown text includes sections on the network structure, the Contrastive Loss, and implementation notes, utilizing LaTeX for the mathematical expressions. You can use this text in Markdown editors that support LaTeX for math rendering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "successful-yacht",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetworkDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # ここでは仮のデータを生成しています\n",
    "        self.data = torch.randn(1000, 3, 64, 64)  # 1000個の3x64x64の画像\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x1 = self.data[index]\n",
    "        # 同じクラスの画像を取得（ここではランダムに選んでいます）\n",
    "        x2 = self.data[np.random.randint(0, len(self.data))]\n",
    "        # ラベル（ここでは0か1でランダムに）\n",
    "        label = np.random.randint(0, 2)\n",
    "        return x1, x2, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "referenced-carolina",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.convnet = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, 5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128 * 13 * 13, 512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(512, 2)\n",
    "        )\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = self.convnet(x)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        out1 = self.forward_one(x1)\n",
    "        out2 = self.forward_one(x2)\n",
    "        return out1, out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "valid-palace",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = nn.functional.pairwise_distance(output1, output2)\n",
    "        \n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      label * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "floral-scotland",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = SiameseNetwork().cuda()\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# データローダーの設定\n",
    "dataset = SiameseNetworkDataset()\n",
    "train_dataloader = DataLoader(dataset, shuffle=True, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "proof-stanley",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.1434\n",
      "Epoch [2/10], Loss: 1.9985\n",
      "Epoch [3/10], Loss: 1.8040\n",
      "Epoch [4/10], Loss: 1.5590\n",
      "Epoch [5/10], Loss: 0.6502\n",
      "Epoch [6/10], Loss: 1.1401\n",
      "Epoch [7/10], Loss: 0.7549\n",
      "Epoch [8/10], Loss: 1.0407\n",
      "Epoch [9/10], Loss: 0.6354\n",
      "Epoch [10/10], Loss: 1.6687\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    for data in train_dataloader:\n",
    "        img1, img2, label = data\n",
    "        img1, img2, label = img1.cuda(), img2.cuda(), label.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output1, output2 = model(img1, img2)\n",
    "        loss = criterion(output1, output2, label.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch [{epoch+1}/10], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alone-stuff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
